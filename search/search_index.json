{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Container Shipment Order Management Service Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to Reefer shipment order subdomain. It is part of the Event Driven Architecture solution implementation . From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipping order issued by a customer who wants to ship fresh goods overseas. The business process is defined here and the event storming analysis in this note . We are also presenting one way of applying Domain Drive Design practice for this subdomain. What you will learn By studying this repository you will be able to learn the following subjects: How to apply domain driven design for a CQRS microservice How to adopt CQRS pattern for the shipping order management How to apply ubiquituous language in the code Develop and deploy a microprofile 2.2 application, using open Liberty, on openshift or kubernetes Requirements The key business requirements we need to support are: Be able to book a fresh product shipment order, including the allocation of the voyage and the assignment of a reefer container to the expected cargo. Be able to understand what happen to the order over time: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? Track key issue or step in the reefer shipment process How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? Be able to support adhoc query on the order that span across subdomains of the shipment domain. What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a fresh food order from california to China? Those requirements force use to consider event sourcing (understanding facts about the order over time) and CQRS patterns to separate queries from command so our architecture will be more flexible and may address different scaling requirements. Applying Domain Driven Design We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach. Review detail implementation considerations The code is the source of truth, but we are providing some simple explanations on how to navigate into the code, and some implementation consideration in this note . Deploy to kubernetes See this note on how to deploy this service with its configuration on Openshift or kubernetes.","title":"Introduction"},{"location":"#reefer-container-shipment-order-management-service","text":"Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to Reefer shipment order subdomain. It is part of the Event Driven Architecture solution implementation . From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipping order issued by a customer who wants to ship fresh goods overseas. The business process is defined here and the event storming analysis in this note . We are also presenting one way of applying Domain Drive Design practice for this subdomain.","title":"Reefer Container Shipment Order Management Service"},{"location":"#what-you-will-learn","text":"By studying this repository you will be able to learn the following subjects: How to apply domain driven design for a CQRS microservice How to adopt CQRS pattern for the shipping order management How to apply ubiquituous language in the code Develop and deploy a microprofile 2.2 application, using open Liberty, on openshift or kubernetes","title":"What you will learn"},{"location":"#requirements","text":"The key business requirements we need to support are: Be able to book a fresh product shipment order, including the allocation of the voyage and the assignment of a reefer container to the expected cargo. Be able to understand what happen to the order over time: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? Track key issue or step in the reefer shipment process How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? Be able to support adhoc query on the order that span across subdomains of the shipment domain. What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a fresh food order from california to China? Those requirements force use to consider event sourcing (understanding facts about the order over time) and CQRS patterns to separate queries from command so our architecture will be more flexible and may address different scaling requirements.","title":"Requirements"},{"location":"#applying-domain-driven-design","text":"We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach.","title":"Applying Domain Driven Design"},{"location":"#review-detail-implementation-considerations","text":"The code is the source of truth, but we are providing some simple explanations on how to navigate into the code, and some implementation consideration in this note .","title":"Review detail implementation considerations"},{"location":"#deploy-to-kubernetes","text":"See this note on how to deploy this service with its configuration on Openshift or kubernetes.","title":"Deploy to kubernetes"},{"location":"build-run/","text":"Build and run the order microservices locally We support different deployment models for event streams and the reefer solution: local with docker-compose, and remote using kubernetes on IBM Cloud (IKS), or on premise with Openshift. When developing the microservice we can access kafka / event streams remote deployed on IBM Cloud or running locally via docker compose. To build and run we are proposing to use some shell scripts. Each script accepts one argument: LOCAL (default if no argument): it uses the kafka image as define in docker compose from the refarch-kc project. IBM_CLOUD : to access event streams as service on IBM Cloud. ICP : to access event streams deployed on Openshift cluster on premise. This argument is used to set environment variables used in the code. In fact the setenv.sh script is used, and is defined in the root refarch-kc project and the scripts folder. Pre-requisites You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work like: Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) For IBMCLOUD, you need to be sure to have an Event Stream service defined (See this note for a simple summary of what to do). If you run IBM Event Streams on openshift cluster on premise servers, be sure to get truststore certificates and API key. Configure the following topics for both microservices: orderCommands , errors , orders . You can use the script createTopics.sh in the refarch-kc project for that or use the Event Streams user interface. Build There are two separate folders to manage the code and scripts for the CQRS command and query parts: order-command-ms order-query-ms You need to build each microservice independently using maven. Each microservice has its own build script to perform the maven package and build the docker image. See scripts folder under each project. Any microservice in this repository can be compiled, unit tested and packaged as war file using maven: mvn package For order-command-ms, the following command will run unit tests and package the war file, then build a docker image cd order - command - ms . / scripts / buildDocker . sh IBMCLOUD If you want to run the integration test you need to do the following: source .. / .. / refarch - kc / scripts / setenv . sh IBMCLOUD mvn install or mvn integration - test For order-query-ms . / scripts / buildDocker . sh IBMCLOUD Note The build scripts test if the javatool docker image exists and if so they use it, otherwise they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase / kcontainer - order - query - ms latest b85b43980f35 531 MB ibmcase / kcontainer - order - command - ms latest Run You can always use the maven command to compile and run liberty server for each project. Before doing so be sure to have set the KAFKA_BROKERS and KAFKA_API_KEY environment variables with the setenv.sh command coming from the refarch-kc project, which should be at the same level in folder hierarchy as this repository. source .. / .. / refarch - kc / scripts / setenv . sh IBMCLOUD mvn install mvn liberty : run - server With docker compose To run the complete solution locally we use docker compose from the root refarch-kc project. docker - compose - f backbone - compose . yml up docker - compose - f kc - solution - compose . yml up And to stop everything: docker - compose - f kc - solution - compose . yml down docker - compose - f backbone - compose . yml down Deploy on kubernetes cluster See this note.","title":"Build and run locally"},{"location":"build-run/#build-and-run-the-order-microservices-locally","text":"We support different deployment models for event streams and the reefer solution: local with docker-compose, and remote using kubernetes on IBM Cloud (IKS), or on premise with Openshift. When developing the microservice we can access kafka / event streams remote deployed on IBM Cloud or running locally via docker compose. To build and run we are proposing to use some shell scripts. Each script accepts one argument: LOCAL (default if no argument): it uses the kafka image as define in docker compose from the refarch-kc project. IBM_CLOUD : to access event streams as service on IBM Cloud. ICP : to access event streams deployed on Openshift cluster on premise. This argument is used to set environment variables used in the code. In fact the setenv.sh script is used, and is defined in the root refarch-kc project and the scripts folder.","title":"Build and run the order microservices locally"},{"location":"build-run/#pre-requisites","text":"You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work like: Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) For IBMCLOUD, you need to be sure to have an Event Stream service defined (See this note for a simple summary of what to do). If you run IBM Event Streams on openshift cluster on premise servers, be sure to get truststore certificates and API key. Configure the following topics for both microservices: orderCommands , errors , orders . You can use the script createTopics.sh in the refarch-kc project for that or use the Event Streams user interface.","title":"Pre-requisites"},{"location":"build-run/#build","text":"There are two separate folders to manage the code and scripts for the CQRS command and query parts: order-command-ms order-query-ms You need to build each microservice independently using maven. Each microservice has its own build script to perform the maven package and build the docker image. See scripts folder under each project. Any microservice in this repository can be compiled, unit tested and packaged as war file using maven: mvn package For order-command-ms, the following command will run unit tests and package the war file, then build a docker image cd order - command - ms . / scripts / buildDocker . sh IBMCLOUD If you want to run the integration test you need to do the following: source .. / .. / refarch - kc / scripts / setenv . sh IBMCLOUD mvn install or mvn integration - test For order-query-ms . / scripts / buildDocker . sh IBMCLOUD Note The build scripts test if the javatool docker image exists and if so they use it, otherwise they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase / kcontainer - order - query - ms latest b85b43980f35 531 MB ibmcase / kcontainer - order - command - ms latest","title":"Build"},{"location":"build-run/#run","text":"You can always use the maven command to compile and run liberty server for each project. Before doing so be sure to have set the KAFKA_BROKERS and KAFKA_API_KEY environment variables with the setenv.sh command coming from the refarch-kc project, which should be at the same level in folder hierarchy as this repository. source .. / .. / refarch - kc / scripts / setenv . sh IBMCLOUD mvn install mvn liberty : run - server","title":"Run"},{"location":"build-run/#with-docker-compose","text":"To run the complete solution locally we use docker compose from the root refarch-kc project. docker - compose - f backbone - compose . yml up docker - compose - f kc - solution - compose . yml up And to stop everything: docker - compose - f kc - solution - compose . yml down docker - compose - f backbone - compose . yml down","title":"With docker compose"},{"location":"build-run/#deploy-on-kubernetes-cluster","text":"See this note.","title":"Deploy on kubernetes cluster"},{"location":"ddd-applied/","text":"Order subdomain - analysis and design The event storming workshop helped us to engage with the business experts and understand the end to end reefer shipping process as well as the events created. The following diagram represents part of this work: (It may have evolved a little bit as we worked with domain experts to learn about the domain) We assume you are now familiar with the event storming methodology as presented in this note . And you have some basic knowledge of domain driven design, from reading books like Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book and reading our summary in this note We recommend to read the note \"From analysis to implementation\" to get a clear understanding of the application design and the ubiquituous language of the reefer shipping domain. In this part we focus on the order sub-domain. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. We are not addressing contract in this solution. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel itineraries. Voyage is an entity grouping the information about the origin harbor close to the pickup address, to a destination harbor the closest to the shipping address. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor. Ubiquitous language Entities and Value Objects Order is the main business entity of this service, and is uniquely identified by its orderID. The orderID is sequentially created when persisting it in its repository. The OrderID will be communicated asynchronously to the customer via a confirmation email. The order request specifies: The pickup location where empty refrigerator container will be loaded The delivery location where the reefer is to be delivered to (we expect this to be in a remote country requiring a sea voyage or itinerary) The type of good with the target temperature to maintain during the travel. The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required All those information are becoming value objects: the different addresses delivery specifications deliver history The delivery history implementation will be support via event sourcing and queries. Aggregate boundaries In Domain-driven design, an aggregate groups related objects as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Shipping Order. The product to ship is part of the aggregate. The Shipping Order is what we will persist in one unique transaction. In this project we focus on the shipping order aggregate with two microservices: one for the write model and one for the queries. Shipment order lifecycle and state change events It is important to assess the main business entity's life cycle to define states and events to be produced as part of the state transition. The following diagram illustrates a potential life cycle for the shipping order aggregate: A shipment order is initially created with an API call made by a customer, or via a user interface the customer uses to enter information (the demonstration user interface is simulating the form entry). When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location, and a Reefer container is found then the order can transition to state=ASSIGNED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester, and the shipping company clerk will work with the customer to find a solution: the process is manual. We assume that the shipping company always has enough container available to meet expected shipment demand. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=IN_TRANSIT . The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state =IN_TRANSIT to state = CONTAINER_ON_SHIP . While the order has state = CONTAINER_ON_SHIP , then we will be receiving temperature information from the Reefer container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming sources of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED . Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED . We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state = REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation. Repositories The Order aggregate has its own repository. Command - Policies and Events When adding commands and business policies to the discovered events, we are able to isolate the following commands and events for the order context. Those command will become verbs for the order command APIs: Place a new shipment order. Modify an order request which could not be booked within the requested time window. Confirm shipping order conditions. Define contract conditions. Assign voyage. Assign reefer container. Cancel order. Initiate land transportation. CQRS and event sourcing From the requirements toget a clear understanding of what happen overtime to the shipping order and the fresh cargo, we will adopt an event based solution with event sourcing capability. As we need to join data between different microservices, and we do not know yet the full scope of complex queries the business team wants to do in the future, we opt to implement CQRS for this order microservice. The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API. User stories The business requirements is presented in this note but we will not implement all this end to end process and all the capabilities we just want to select the following user stories: As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. (the contract creation is out of scope, but the list of orders is in) As a shipping company manager, I want to get a specific order knowing its unique identifier so that I can review the data and know the current status. As a shipping company manager, I want to update the status of an order, cancel it or complete it As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. As a shipping company manager, I want to review the reefer containers allocated to the order because I'm curious As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations. At this stage we have identified the bounded context, the aggregates, the commands and events we need to implement for this order management service. The implementation involves at least two microservices, one of the command and one for the query. The implementation details are explained in this note.","title":"Order subdomain - analysis and design"},{"location":"ddd-applied/#order-subdomain-analysis-and-design","text":"The event storming workshop helped us to engage with the business experts and understand the end to end reefer shipping process as well as the events created. The following diagram represents part of this work: (It may have evolved a little bit as we worked with domain experts to learn about the domain) We assume you are now familiar with the event storming methodology as presented in this note . And you have some basic knowledge of domain driven design, from reading books like Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book and reading our summary in this note We recommend to read the note \"From analysis to implementation\" to get a clear understanding of the application design and the ubiquituous language of the reefer shipping domain. In this part we focus on the order sub-domain. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. We are not addressing contract in this solution. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel itineraries. Voyage is an entity grouping the information about the origin harbor close to the pickup address, to a destination harbor the closest to the shipping address. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor.","title":"Order subdomain - analysis and design"},{"location":"ddd-applied/#ubiquitous-language","text":"","title":"Ubiquitous language"},{"location":"ddd-applied/#entities-and-value-objects","text":"Order is the main business entity of this service, and is uniquely identified by its orderID. The orderID is sequentially created when persisting it in its repository. The OrderID will be communicated asynchronously to the customer via a confirmation email. The order request specifies: The pickup location where empty refrigerator container will be loaded The delivery location where the reefer is to be delivered to (we expect this to be in a remote country requiring a sea voyage or itinerary) The type of good with the target temperature to maintain during the travel. The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required All those information are becoming value objects: the different addresses delivery specifications deliver history The delivery history implementation will be support via event sourcing and queries.","title":"Entities and Value Objects"},{"location":"ddd-applied/#aggregate-boundaries","text":"In Domain-driven design, an aggregate groups related objects as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Shipping Order. The product to ship is part of the aggregate. The Shipping Order is what we will persist in one unique transaction. In this project we focus on the shipping order aggregate with two microservices: one for the write model and one for the queries.","title":"Aggregate boundaries"},{"location":"ddd-applied/#shipment-order-lifecycle-and-state-change-events","text":"It is important to assess the main business entity's life cycle to define states and events to be produced as part of the state transition. The following diagram illustrates a potential life cycle for the shipping order aggregate: A shipment order is initially created with an API call made by a customer, or via a user interface the customer uses to enter information (the demonstration user interface is simulating the form entry). When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location, and a Reefer container is found then the order can transition to state=ASSIGNED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester, and the shipping company clerk will work with the customer to find a solution: the process is manual. We assume that the shipping company always has enough container available to meet expected shipment demand. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=IN_TRANSIT . The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state =IN_TRANSIT to state = CONTAINER_ON_SHIP . While the order has state = CONTAINER_ON_SHIP , then we will be receiving temperature information from the Reefer container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming sources of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED . Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED . We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state = REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation.","title":"Shipment order lifecycle and state change events"},{"location":"ddd-applied/#repositories","text":"The Order aggregate has its own repository.","title":"Repositories"},{"location":"ddd-applied/#command-policies-and-events","text":"When adding commands and business policies to the discovered events, we are able to isolate the following commands and events for the order context. Those command will become verbs for the order command APIs: Place a new shipment order. Modify an order request which could not be booked within the requested time window. Confirm shipping order conditions. Define contract conditions. Assign voyage. Assign reefer container. Cancel order. Initiate land transportation.","title":"Command - Policies and Events"},{"location":"ddd-applied/#cqrs-and-event-sourcing","text":"From the requirements toget a clear understanding of what happen overtime to the shipping order and the fresh cargo, we will adopt an event based solution with event sourcing capability. As we need to join data between different microservices, and we do not know yet the full scope of complex queries the business team wants to do in the future, we opt to implement CQRS for this order microservice. The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API.","title":"CQRS and event sourcing"},{"location":"ddd-applied/#user-stories","text":"The business requirements is presented in this note but we will not implement all this end to end process and all the capabilities we just want to select the following user stories: As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. (the contract creation is out of scope, but the list of orders is in) As a shipping company manager, I want to get a specific order knowing its unique identifier so that I can review the data and know the current status. As a shipping company manager, I want to update the status of an order, cancel it or complete it As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. As a shipping company manager, I want to review the reefer containers allocated to the order because I'm curious As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations. At this stage we have identified the bounded context, the aggregates, the commands and events we need to implement for this order management service. The implementation involves at least two microservices, one of the command and one for the query. The implementation details are explained in this note.","title":"User stories"},{"location":"deployments/","text":"Deployments Be sure to have read the build and run article before . Deployment prerequisites Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider. Get the API key from the user interface using an administrator user. Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Note The name of those secrets are used in the Helm chart values.yaml and deployment.yaml files of each project. The helm template command will generate service and deployment yaml files with the good value for each chart. (See section below ) kafka : brokersConfigMap : kafka - brokers eventstreams : enabled : true apikeyConfigMap : eventstreams - apikey truststoreRequired : true truststorePath : /config/resources/security/ es - ssl truststoreFile : es - cert . jks truststoreSecret : es - truststore - jks truststorePassword : changeit Deploy to IKS Be sure to have created an IBM kubernetes service cluster (See this lab for detail) Once the two docker images are built, upload them to the IKS private registry docker push us . icr . io / ibmcaseeda / kcontainer - order - query - ms docker push us . icr . io / ibmcaseeda / kcontainer - order - command - ms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order - command - ms . / scripts / deployHelm MINIKUBE cd order - query - ms . / scripts / deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h Deploy to OpenShift Container Platform (OCP) Deploy to OCP 3.11 Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) If desired, create a non-default Service Account for usage of deploying and running the Reefer Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges. Perform the following for both order-command-ms and order-query-ms microservices: Build and push the Docker image by one of the two options below: Create a Jenkins project, pointing to the remote GitHub repository for the order-command and order-query microservices, and manually creating the necessary parameters. Refer to the order-command Jenkinsfile.NoKubernetesPlugin or order-query Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): docker build -t <private-registry>/<image-namespace>/order-command-ms:latest order-command-ms/ docker build -t <private-registry>/<image-namespace>/order-query-ms:latest order-query-ms/ docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template for both order-command and order-query : Parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms Deploy application using oc apply : oc apply -f templates/ordercommandms/templates","title":"Deploy to Kubernetes cluster"},{"location":"deployments/#deployments","text":"Be sure to have read the build and run article before .","title":"Deployments"},{"location":"deployments/#deployment-prerequisites","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider. Get the API key from the user interface using an administrator user. Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Note The name of those secrets are used in the Helm chart values.yaml and deployment.yaml files of each project. The helm template command will generate service and deployment yaml files with the good value for each chart. (See section below ) kafka : brokersConfigMap : kafka - brokers eventstreams : enabled : true apikeyConfigMap : eventstreams - apikey truststoreRequired : true truststorePath : /config/resources/security/ es - ssl truststoreFile : es - cert . jks truststoreSecret : es - truststore - jks truststorePassword : changeit","title":"Deployment prerequisites"},{"location":"deployments/#deploy-to-iks","text":"Be sure to have created an IBM kubernetes service cluster (See this lab for detail) Once the two docker images are built, upload them to the IKS private registry docker push us . icr . io / ibmcaseeda / kcontainer - order - query - ms docker push us . icr . io / ibmcaseeda / kcontainer - order - command - ms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order - command - ms . / scripts / deployHelm MINIKUBE cd order - query - ms . / scripts / deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h","title":"Deploy to IKS"},{"location":"deployments/#deploy-to-openshift-container-platform-ocp","text":"","title":"Deploy to OpenShift Container Platform (OCP)"},{"location":"deployments/#deploy-to-ocp-311","text":"Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) If desired, create a non-default Service Account for usage of deploying and running the Reefer Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges. Perform the following for both order-command-ms and order-query-ms microservices: Build and push the Docker image by one of the two options below: Create a Jenkins project, pointing to the remote GitHub repository for the order-command and order-query microservices, and manually creating the necessary parameters. Refer to the order-command Jenkinsfile.NoKubernetesPlugin or order-query Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): docker build -t <private-registry>/<image-namespace>/order-command-ms:latest order-command-ms/ docker build -t <private-registry>/<image-namespace>/order-query-ms:latest order-query-ms/ docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template for both order-command and order-query : Parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (only required if pulling from an external private registry) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms Deploy application using oc apply : oc apply -f templates/ordercommandms/templates","title":"Deploy to OCP 3.11"},{"location":"implementation-considerations/","text":"As introduced in the solution design note the order entity life cycle looks like in the following diagram: The order microservice supports the implementations of this life cycle, using event sourcing and CQRS pattern. With CQRS , we separate the 'write model' from the 'read model'. The Command microservice implements the 'write model' and exposes a set of REST end points for Creating Order, Updating, Deleting Order and getting Order per ID. The query service will address more complex queries to support adhoc business requirements and most likely will need to join data between different entities like the order, the containers and the voyages. So we have two Java projects to support each service implementation. Each service is packaged as container and deployable to Kubernetes. Order command microservice Order query microservice As some requirements are related to historical query, using an event approach, we need to keep all the events related to what happens to the order. Instead of implementing a complex logic with the query and command services, the event sourcing is supported by using Kafka topics. The following diagram illustrates the CQRS and event sourcing applied to the order management service. Client to the REST api, like a back end for front end app, performs a HTTP POST operation with the order data. The command generates events and persists order on its own data source. The query part is an event consumer and defines its own data projections to support the different queries: The datasource at the command level, may not be necessary, but we want to illustrate here the fact that it is possible to have a SQL based database or a document oriented database to keep the order last state: a call to get /orders/{id} will return the current order state. For the query part, the projection can be kept in memory or persisted on its own data store. The decision, to go for in memory or to use a database, depends upon the amount of data to join, and the persitence time horizon set at the Kafka topic level. In case of problem or while starting, an event driven service may always rebuild its view by re-reading the topic from the beginning. As the BFF still needs to get order by ID or perform complex queries, it has to access the order service using HTTP, therefore we have prefered to use one communication protocol. The following sequence diagram illustrates the relationships between the components over time for the new shipping order processing: To avoid transaction between the database update and the event published, the choice is to publish the command event to a specific kafka topic ( orderCommands ) as early as the shipping order data is received and use a consumer inside the command service to load the data and save to the database. The kafka topic act as a source of trust for this service. This is illustrated in this article. The following diagram illustrates the update shipping oder flow, and the call to a complex query from a remote client going directly to the query microservice: Code structure For the order command microservice we implemented a domain driven design structure and try to use the ubiquituous language in the code. The first difficult practice it to well organize the code: The layering approach is used: Core is the building blocks not specific to any domain or technology, containing generic building blocks like Lists, Maps, Case Classes, Actors and Lenses. Infrastructure contains adapters for various technologies such as databases, user interface and external services. Domain is where all business logic resides with classes and methods named using the ubiquitous language for the domain. The business logic and rules go there. It has access to infrastructure and core. The App layer, acts as the entry point to the domain, using Domain terms and objects from the Domain. App includes APIs. API should only expose immutable objects to prevent developers from using the exposed objects to gain access to the underlying domain, thus manipulating the domain. A given level can see only the components of its layer or lower App layer As defined by the list of commands to implements, most those operations are defined in an API. The unique REST resource is the class: ShippingOrderResource When the application starts there is a ServletContextListener implementation class started to create kafka consumers. When consumers reach an issue to get events, they create an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop. Domain layer The domain layer implements the business logic, defines the main business entities and value objects, and defines services used by app layer. Data Model We have identified aggregates, entities, value objects and business policies and rules. Those elements help us to build our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what to persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class ShippinOrder { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may use relational database and may have information like Address table, ShippingOrder table and ReeferOrder join table. Main classes of the command service ShippingOrderResource is the REST api resource class for /orders POST, PUT and GET. It is part of the app layer. ShippingOrderService is part of the domain layer, and groups the service operation to manage a shipping order. ShippingOrderRepository This is an interface, but there is a mockup implementation to keep the data in memory. It is part of the infrastructure layer. OrderCommandProducer is part of the infrastructure, and produce command event to sequence the operation between the messaging and the database. OrderCommandAgent is also part of the infrastructure, with its counter part class the OrderCommandRunner which loop on polling message from the orderCommands topic. OrderEventProducer is producing events for other microservice to consume. The following sequence diagram illustrates how those components are working together: Infrastructure Layer This layer includes Repository and Agents consumer of events as well as event emitters for the command events and the factual events. Normally events are factuals per design. This is just a little bit of semantic play here, as we are using command events to manage data integrity. Command Events We did use command event to avoid XA transaction between kafka and database. So we use a 'private' topic: orderCommands to queue the commands to create or update a Shipping order. The command microservice has an agent to consume such events, get the shipping order data and save to the database. See this class: OrderCommandAgent Factual Events On the event side, we may generate OrderCreated, OrderCancelled,... so other services can act on those shipping order changes. The event payload will depend of the event type. So we first define a base class for timestamp, version and type. Then each specific event define the payload. We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEventBase { timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. Using Avro and schema registry will help managing different version of the payload. class ReeferAssignedEvent extends OrderEventBase { ReeferAssignmentPayload payload ; } In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With event driven microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard, and adopting event sourcing pattern and an event backbone, like kafka, which becomes the source of truth.","title":"Implementation considerations"},{"location":"implementation-considerations/#code-structure","text":"For the order command microservice we implemented a domain driven design structure and try to use the ubiquituous language in the code. The first difficult practice it to well organize the code: The layering approach is used: Core is the building blocks not specific to any domain or technology, containing generic building blocks like Lists, Maps, Case Classes, Actors and Lenses. Infrastructure contains adapters for various technologies such as databases, user interface and external services. Domain is where all business logic resides with classes and methods named using the ubiquitous language for the domain. The business logic and rules go there. It has access to infrastructure and core. The App layer, acts as the entry point to the domain, using Domain terms and objects from the Domain. App includes APIs. API should only expose immutable objects to prevent developers from using the exposed objects to gain access to the underlying domain, thus manipulating the domain. A given level can see only the components of its layer or lower","title":"Code structure"},{"location":"implementation-considerations/#app-layer","text":"As defined by the list of commands to implements, most those operations are defined in an API. The unique REST resource is the class: ShippingOrderResource When the application starts there is a ServletContextListener implementation class started to create kafka consumers. When consumers reach an issue to get events, they create an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop.","title":"App layer"},{"location":"implementation-considerations/#domain-layer","text":"The domain layer implements the business logic, defines the main business entities and value objects, and defines services used by app layer.","title":"Domain layer"},{"location":"implementation-considerations/#data-model","text":"We have identified aggregates, entities, value objects and business policies and rules. Those elements help us to build our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what to persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class ShippinOrder { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may use relational database and may have information like Address table, ShippingOrder table and ReeferOrder join table.","title":"Data Model"},{"location":"implementation-considerations/#main-classes-of-the-command-service","text":"ShippingOrderResource is the REST api resource class for /orders POST, PUT and GET. It is part of the app layer. ShippingOrderService is part of the domain layer, and groups the service operation to manage a shipping order. ShippingOrderRepository This is an interface, but there is a mockup implementation to keep the data in memory. It is part of the infrastructure layer. OrderCommandProducer is part of the infrastructure, and produce command event to sequence the operation between the messaging and the database. OrderCommandAgent is also part of the infrastructure, with its counter part class the OrderCommandRunner which loop on polling message from the orderCommands topic. OrderEventProducer is producing events for other microservice to consume. The following sequence diagram illustrates how those components are working together:","title":"Main classes of the command service"},{"location":"implementation-considerations/#infrastructure-layer","text":"This layer includes Repository and Agents consumer of events as well as event emitters for the command events and the factual events. Normally events are factuals per design. This is just a little bit of semantic play here, as we are using command events to manage data integrity.","title":"Infrastructure Layer"},{"location":"implementation-considerations/#command-events","text":"We did use command event to avoid XA transaction between kafka and database. So we use a 'private' topic: orderCommands to queue the commands to create or update a Shipping order. The command microservice has an agent to consume such events, get the shipping order data and save to the database. See this class: OrderCommandAgent","title":"Command Events"},{"location":"implementation-considerations/#factual-events","text":"On the event side, we may generate OrderCreated, OrderCancelled,... so other services can act on those shipping order changes. The event payload will depend of the event type. So we first define a base class for timestamp, version and type. Then each specific event define the payload. We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEventBase { timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. Using Avro and schema registry will help managing different version of the payload. class ReeferAssignedEvent extends OrderEventBase { ReeferAssignmentPayload payload ; } In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With event driven microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard, and adopting event sourcing pattern and an event backbone, like kafka, which becomes the source of truth.","title":"Factual Events"}]}